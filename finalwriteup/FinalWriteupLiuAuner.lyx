#LyX file created by tex2lyx 2.0.6dev
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble

\usepackage{fullpage}% need for subequations
% need for figures
% useful for program listings
\usepackage{color}% use if color is used in text
\usepackage{subfigure}% use for side-by-side figures
% use for hypertext links, including those to external documents and URLs

\title{CMSC 12300 Final Project}
\author{Andy Liu and Nelson Auner}

\end_preamble
\use_default_options false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\paperfontsize 11
\spacing single
\use_hyperref 1
\pdf_bookmarks 1
\pdf_bookmarksnumbered 0
\pdf_bookmarksopen 0
\pdf_bookmarksopenlevel 1
\pdf_breaklinks 0
\pdf_pdfborder 0
\pdf_colorlinks 0
\pdf_backref section
\pdf_pdfusetitle 1
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash
maketitle
\end_layout

\end_inset

 
\end_layout

\begin_layout Section

Introduction
\end_layout

\begin_layout Standard


\begin_inset ERT
status collapsed

\begin_layout Standard

%%Maybe move this subsection out of the introduction
\end_layout

\begin_layout Standard


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

Logistic regression of census data
\end_layout

\begin_layout Standard

Our project investigates predicting whether a household self reports that its income is over or under $50,000, based on census data. We aim to find the best classification method within the subset of logistic regression classifiers. Logistic regression transforms the predictors to 
\begin_inset Formula \begin{equation}
\pi(X) = \frac{e^{(X\beta)}}{1+e^{(X\beta)}}
\end{equation}
\end_inset

this projects the space of 
\begin_inset Formula $X \beta$
\end_inset

 to the interval 
\begin_inset Formula $[0,1]$
\end_inset

, allowing us to view the result as the probability that our outcome is 1. Although more complicated than a basic regression mode 
\begin_inset Formula $Y = X\beta$
\end_inset

, logistic regression allows us to predict a binary outcome, while resulting in more interpretable coefficients 
\begin_inset Formula $\beta$
\end_inset

 than black box methods such as neural networks or random forest. 
\end_layout

\begin_layout Subsection

Goals and summary of results
\end_layout

\begin_layout Standard

Our main goals, put succintly, were to find and compare the best prediction method by testing various combination of predictors. For each prediction method, we wanted to quantify the effectiveness of that algorithm. Given the large proportion of observations with missing values, we also wanted to examine the effects of filling in missing observations with values.
\end_layout

\begin_layout Section

Description of the Data set
\end_layout

\begin_layout Standard

Our analysis is on the Census-Income (KDD) Data Set, a classic machine learning dataset as well as applicable machine The census data we used is made freely available from 
\begin_inset CommandInset href
LatexCommand href
name ""
target "http://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29}{the UC-Irvine Machine Learning Repository}  The most pertinant characteristic of the data set is that the "over 50K" variable only takes on a value of true for 6.2% of the observations. As we noted in our original proposal, this means that a majority classifiers--predicting that income is under 50K for all observations, would have been "93.8% accurate". However, this is not a useful predictor, since we do not know if a set of new observations would be reprentative at all of our current data. In addition, it's important to consider weighting the relative importance of assigning a false positive vs. a false negative: for a business/marketing application of the data, it may be more important to predict all of the observations with income > $50K, than correctly predict observation under $50K. In this sense, the majority classifier is undesireable, asit predicts no observations with income > $50K, regardless of information.  section{Initial analysis} Our inital analysis was done in R, and much work was required to find and adapt the necessary statistical methods for use in python.  We used the Akaike Information Criteria (AIC), defined as $AIC = 2k - 2ln(L)$, where $k$ is the number of the parameters in the model, and $L$ is the maximized value of the likelihood function - in our case, the logistical regression equation defined above. By maximizing AIC, we produced a subset of predictors that AIC evaluated as the "best" predictor sets. We noticed that among single-variable predictive methods, the education variable was one of the most effective, with a false positive error rate of 5% false negative error rate of .7%. However, these rates are not accurate measures of performance, because we are testing our predictor with the same data we used to train.  end{document}"

\end_inset


\end_layout

\end_body
\end_document
